{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc387a0",
   "metadata": {},
   "source": [
    "# PySpark `groupBy()` \n",
    "        by Aishwarya Raut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5a701",
   "metadata": {},
   "source": [
    "Similar to SQL `Group By` clause, PySpark `groupBy()` function is used to collect the identical data into groups on DF and perform count, sum, avg, min, max functions on the grouped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29abe84b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:16:13.496997Z",
     "start_time": "2023-11-30T11:16:12.096333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+---------+----------+-------------+-----------------+----------------+\n",
      "|Row|      Day|Day_Of_Week|     Date|Page_Loads|Unique_Visits|First_Time_Visits|Returning_Visits|\n",
      "+---+---------+-----------+---------+----------+-------------+-----------------+----------------+\n",
      "|  1|   Sunday|          1|9/14/2014|      2146|         1582|             1430|             152|\n",
      "|  2|   Monday|          2|9/15/2014|      3621|         2528|             2297|             231|\n",
      "|  3|  Tuesday|          3|9/16/2014|      3698|         2630|             2352|             278|\n",
      "|  4|Wednesday|          4|9/17/2014|      3667|         2614|             2327|             287|\n",
      "|  5| Thursday|          5|9/18/2014|      3316|         2366|             2130|             236|\n",
      "+---+---------+-----------+---------+----------+-------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"PySpark\").getOrCreate()\n",
    "\n",
    "\n",
    "# read csv file\n",
    "file_path = \"C:\\\\Users\\\\pcc\\\\Desktop\\\\daily-website-visitors.csv\"\n",
    "df=spark.read.csv(file_path,header=True,inferSchema=True)\n",
    "\n",
    "df=df.withColumnsRenamed({\"Day.Of.Week\":\"Day_Of_Week\",\"Page.Loads\":\"Page_Loads\",\n",
    "                          \"Unique.Visits\":\"Unique_Visits\",\"First.Time.Visits\":\"First_Time_Visits\",\n",
    "                          \"Returning.Visits\":\"Returning_Visits\"})\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8bf4d",
   "metadata": {},
   "source": [
    "# 1. GroupBy() Syntax & Usage\n",
    "\n",
    "**Syntax**\n",
    "DataFrame.groupBy(*cols)\n",
    "\n",
    "When we perform `groupBy()`  on pyspark DataFrame, it returns `GroupedData` object which contains below aggregate functions.\n",
    "\n",
    "`count()`, \n",
    "`mean()`,\n",
    "`max()`,\n",
    "`min()`,\n",
    "`sum()`,\n",
    "`avg()`,\n",
    "`agg()`,\n",
    "`pivot()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bf501",
   "metadata": {},
   "source": [
    "# 2. PySpark groupBy on DataFrame Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b188f169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:16:59.209089Z",
     "start_time": "2023-11-30T11:16:54.517911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|Day      |sum(Page_Loads)|\n",
      "+---------+---------------+\n",
      "|Wednesday|1517114        |\n",
      "|Tuesday  |1536154        |\n",
      "|Friday   |1149437        |\n",
      "|Thursday |1437269        |\n",
      "|Saturday |772817         |\n",
      "|Monday   |1502161        |\n",
      "|Sunday   |1006564        |\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").sum(\"Page_Loads\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba14f1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:17:34.596122Z",
     "start_time": "2023-11-30T11:17:33.302068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Day      |count|\n",
      "+---------+-----+\n",
      "|Wednesday|310  |\n",
      "|Tuesday  |310  |\n",
      "|Friday   |309  |\n",
      "|Thursday |309  |\n",
      "|Saturday |309  |\n",
      "|Monday   |310  |\n",
      "|Sunday   |310  |\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19f30221",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:17:49.338565Z",
     "start_time": "2023-11-30T11:17:47.975846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|Day      |min(Page_Loads)|\n",
      "+---------+---------------+\n",
      "|Wednesday|1381           |\n",
      "|Tuesday  |1644           |\n",
      "|Friday   |1017           |\n",
      "|Thursday |1002           |\n",
      "|Saturday |1115           |\n",
      "|Monday   |1609           |\n",
      "|Sunday   |1326           |\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").min(\"Page_Loads\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b6d94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:18:03.771892Z",
     "start_time": "2023-11-30T11:18:02.668478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|Day      |max(Page_Loads)|\n",
      "+---------+---------------+\n",
      "|Wednesday|7984           |\n",
      "|Tuesday  |7714           |\n",
      "|Friday   |5735           |\n",
      "|Thursday |7250           |\n",
      "|Saturday |4614           |\n",
      "|Monday   |7840           |\n",
      "|Sunday   |5979           |\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").max(\"Page_Loads\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608f8659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:18:16.377460Z",
     "start_time": "2023-11-30T11:18:15.196281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Day      |avg(Page_Loads)   |\n",
      "+---------+------------------+\n",
      "|Wednesday|4893.916129032258 |\n",
      "|Tuesday  |4955.335483870967 |\n",
      "|Friday   |3719.8608414239484|\n",
      "|Thursday |4651.355987055016 |\n",
      "|Saturday |2501.0258899676373|\n",
      "|Monday   |4845.68064516129  |\n",
      "|Sunday   |3246.9806451612903|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").avg(\"Page_Loads\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a46d9e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:18:37.482858Z",
     "start_time": "2023-11-30T11:18:36.599740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Day      |avg(Page_Loads)   |\n",
      "+---------+------------------+\n",
      "|Wednesday|4893.916129032258 |\n",
      "|Tuesday  |4955.335483870967 |\n",
      "|Friday   |3719.8608414239484|\n",
      "|Thursday |4651.355987055016 |\n",
      "|Saturday |2501.0258899676373|\n",
      "|Monday   |4845.68064516129  |\n",
      "|Sunday   |3246.9806451612903|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").mean(\"Page_Loads\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6577e19",
   "metadata": {},
   "source": [
    "# 3. Using Multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409b1417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:45:50.915389Z",
     "start_time": "2023-11-30T11:45:50.223307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+---------------------+\n",
      "|Day      |sum(First_Time_Visits)|sum(Returning_Visits)|\n",
      "+---------+----------------------+---------------------+\n",
      "|Wednesday|897602                |188022               |\n",
      "|Tuesday  |907752                |189429               |\n",
      "|Friday   |668805                |149047               |\n",
      "|Thursday |848921                |179293               |\n",
      "|Saturday |456449                |95656                |\n",
      "|Monday   |886036                |186076               |\n",
      "|Sunday   |604198                |121596               |\n",
      "+---------+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").sum(\"First_Time_Visits\",\n",
    "                      \"Returning_Visits\")\\\n",
    "                .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb8aea3",
   "metadata": {},
   "source": [
    "# 4. Running more aggregates at a time\n",
    "\n",
    "Using agg() aggregate function we can calculate many aggregations at a time on a single statement using SQL functions sum(), avg(), min(), max() mean() e.t.c. In order to use these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42563f89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:37:18.693600Z",
     "start_time": "2023-11-30T11:37:17.672782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "|Day      |sum_first_time_visitor|avg_First_Time_Visits|Sum_First_Time_Visits|Max_First_Time_Visits|\n",
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "|Wednesday|897602                |2895.490322580645    |897602               |4616                 |\n",
      "|Tuesday  |907752                |2928.232258064516    |907752               |4500                 |\n",
      "|Friday   |668805                |2164.4174757281553   |668805               |3592                 |\n",
      "|Thursday |848921                |2747.3171521035597   |848921               |4213                 |\n",
      "|Saturday |456449                |1477.1812297734627   |456449               |2932                 |\n",
      "|Monday   |886036                |2858.18064516129     |886036               |4569                 |\n",
      "|Sunday   |604198                |1949.0258064516129   |604198               |3393                 |\n",
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg, max ,col\n",
    "df.groupBy(\"Day\").agg(sum(\"First_Time_Visits\").alias(\"sum_first_time_visitor\"),\n",
    "                     avg(\"First_Time_Visits\").alias(\"avg_First_Time_Visits\"),\n",
    "                     sum(\"First_Time_Visits\").alias(\"Sum_First_Time_Visits\"),\n",
    "                     max(\"First_Time_Visits\").alias(\"Max_First_Time_Visits\")\n",
    "                     ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ef8bc",
   "metadata": {},
   "source": [
    "# 5. Using filter on aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70406fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T11:38:46.544609Z",
     "start_time": "2023-11-30T11:38:45.321052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "|Day      |sum_first_time_visitor|avg_First_Time_Visits|Sum_First_Time_Visits|Max_First_Time_Visits|\n",
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "|Wednesday|897602                |2895.490322580645    |897602               |4616                 |\n",
      "|Tuesday  |907752                |2928.232258064516    |907752               |4500                 |\n",
      "|Friday   |668805                |2164.4174757281553   |668805               |3592                 |\n",
      "|Thursday |848921                |2747.3171521035597   |848921               |4213                 |\n",
      "|Saturday |456449                |1477.1812297734627   |456449               |2932                 |\n",
      "|Monday   |886036                |2858.18064516129     |886036               |4569                 |\n",
      "|Sunday   |604198                |1949.0258064516129   |604198               |3393                 |\n",
      "+---------+----------------------+---------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Day\").agg(sum(\"First_Time_Visits\").alias(\"sum_first_time_visitor\"),\n",
    "                     avg(\"First_Time_Visits\").alias(\"avg_First_Time_Visits\"),\n",
    "                     sum(\"First_Time_Visits\").alias(\"Sum_First_Time_Visits\"),\n",
    "                     max(\"First_Time_Visits\").alias(\"Max_First_Time_Visits\")\n",
    "                     ).where(col(\"Sum_First_Time_Visits\")>=500).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
