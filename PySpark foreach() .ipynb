{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80ad22b",
   "metadata": {},
   "source": [
    "PySpark foreach() is an action operation that is available in RDD, DataFram to iterate/loop over each element in the DataFrmae, It is similar to for with advanced concepts. This is different than other actions as foreach() function doesn’t return a value instead it executes the input function on each element of an RDD, DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ceaab1",
   "metadata": {},
   "source": [
    "# 1. PySpark DataFrame foreach()\n",
    "\n",
    "## 1.1 foreach() Syntax\n",
    "\n",
    "`DataFrame.foreach(f)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4387095",
   "metadata": {},
   "source": [
    "## 1.2 PySpark foreach() Usage\n",
    "\n",
    "When foreach() applied on PySpark DataFrame, it executes a function specified in for each element of DataFrame. This operation is mainly used if you wanted to manipulate accumulators, save the DataFrame results to RDBMS tables, Kafka topics, and other external sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479299f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:25:19.267443Z",
     "start_time": "2023-12-02T10:25:19.123143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SP') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5090ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:25:55.230511Z",
     "start_time": "2023-12-02T10:25:55.215519Z"
    }
   },
   "outputs": [],
   "source": [
    "# foreach() example\n",
    "def f(df):\n",
    "    print(df.Seqno)\n",
    "    \n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach() with accumulator example\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "df.foreach(lambda x:accum.add(int(x.Seqno)))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df8105",
   "metadata": {},
   "source": [
    "# 2. PySpark RDD foreach() Usage\n",
    "\n",
    "## 2.1 Syntax\n",
    "\n",
    "### Syntax\n",
    "\n",
    "`RDD.foreach(f: Callable[[T], None]) → None`\n",
    "\n",
    "## 2.2 RDD foreach() Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach() with RDD example\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value) #Accessed by driver\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
