{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875358e5",
   "metadata": {},
   "source": [
    "# PySpark `transform()` function -  \n",
    "        By Aishwarya Raut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a4e14",
   "metadata": {},
   "source": [
    "PySpark provides two `transform()` functions one with DF and another in `pyspark.sql.functions`.\n",
    "\n",
    "> `pyspark.sql.DataFrame.transform()`\n",
    "> `pyspark.sql.functions.transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c822c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T10:14:00.755868Z",
     "start_time": "2023-12-01T10:12:58.811183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SP') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = ((\"Java\",4000,5), \\\n",
    "    (\"Python\", 4600,10),  \\\n",
    "    (\"Scala\", 4100,15),   \\\n",
    "    (\"Scala\", 4500,15),   \\\n",
    "    (\"PHP\", 3000,20),  \\\n",
    "  )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67344f0e",
   "metadata": {},
   "source": [
    "# 1. PySpark DataFrame.transform()\n",
    "\n",
    "The pyspark.sql.DataFrame.transform() is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "\n",
    "This function always returns the same number of rows that exists on the input PySpark DataFrame.\n",
    "\n",
    "## 1.1 Syntax \n",
    "`pyspark.sql.DataFrame.transform()`\n",
    "\n",
    "`DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n",
    "`\n",
    "\n",
    "`func` – Custom function to call.\n",
    "`*args` – Arguments to pass to func.\n",
    "`*kwargs` – Keyword arguments to pass to func.\n",
    "\n",
    "## 1.2 Create Custom Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc6dd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T12:02:17.519364Z",
     "start_time": "2023-12-01T12:02:17.494378Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
    "\n",
    "def reduce_price(df,reduceBy):\n",
    "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discount_fee\",\n",
    "                        df.new_fee-(df.new_fee * df.discount)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b69e6c",
   "metadata": {},
   "source": [
    "# 1.3 PySpark Apply DataFrame.transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246718a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T12:03:31.667861Z",
     "start_time": "2023-12-01T12:03:30.690761Z"
    }
   },
   "outputs": [],
   "source": [
    "df2=df.transform(to_upper_str_columns).transform(reduce_price,1000).transform(apply_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ae2788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T12:03:51.119754Z",
     "start_time": "2023-12-01T12:03:51.106763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- fee: long (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- new_fee: long (nullable = true)\n",
      " |-- discount_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b05aea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T12:05:12.531681Z",
     "start_time": "2023-12-01T12:05:12.345405Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom function\n",
    "def select_columns(df):\n",
    "    return df.select(\"CourseName\",\"discount_fee\")\n",
    "\n",
    "# Chain transformations\n",
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price,1000) \\\n",
    "        .transform(apply_discount) \\\n",
    "        .transform(select_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb14569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T12:05:21.579884Z",
     "start_time": "2023-12-01T12:05:21.567890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CourseName: string (nullable = true)\n",
      " |-- discount_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eb26ab",
   "metadata": {},
   "source": [
    "# 2. PySpark sql.functions.transform()\n",
    "\n",
    "The `PySpark sql.functions.transform()` is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType.\n",
    "\n",
    "Following is the syntax of the `pyspark.sql.functions.transform()` function\n",
    "\n",
    "\n",
    "# Syntax\n",
    "`pyspark.sql.functions.transform(col, f)`\n",
    "The following are the parameters:\n",
    "\n",
    "`col` – ArrayType column\n",
    "\n",
    "`f` – Optional. Function to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54ccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with Array\n",
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# using transform() function\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "df.select(transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")) \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
