{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc836c4",
   "metadata": {},
   "source": [
    "`pyspark.sql.DataFrame.sample()`\n",
    "\n",
    "`pyspark.sql.DataFrame.sampleBy()`\n",
    "\n",
    "`RDD.sample()`\n",
    "\n",
    "`RDD.takeSample()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a696a6ae",
   "metadata": {},
   "source": [
    "# 1. PySpark SQL sample() Usage & Examples\n",
    "\n",
    "`sample(withReplacement, fraction, seed=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a215d415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:54:55.953124Z",
     "start_time": "2023-12-02T10:54:53.780137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=18), Row(id=40), Row(id=57), Row(id=70), Row(id=86), Row(id=88)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SP\").getOrCreate()\n",
    "\n",
    "df=spark.range(100)\n",
    "print(df.sample(0.06).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0c55f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:44:40.994481Z",
     "start_time": "2023-12-02T10:44:40.239650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=11), Row(id=13), Row(id=20), Row(id=35), Row(id=46), Row(id=49), Row(id=52), Row(id=67), Row(id=71), Row(id=95), Row(id=97)]\n",
      "----------------------\n",
      "[Row(id=11), Row(id=13), Row(id=20), Row(id=35), Row(id=46), Row(id=49), Row(id=52), Row(id=67), Row(id=71), Row(id=95), Row(id=97)]\n",
      "----------------------\n",
      "[Row(id=7), Row(id=14), Row(id=68), Row(id=69), Row(id=88), Row(id=96)]\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "print(df.sample(0.1,7).collect())\n",
    "print(\"----------------------\")\n",
    "print(df.sample(0.1,7).collect())\n",
    "print(\"----------------------\")\n",
    "print(df.sample(0.1,13).collect())\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad94d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:46:53.076841Z",
     "start_time": "2023-12-02T10:46:49.823040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=4), Row(id=8), Row(id=13), Row(id=14), Row(id=19), Row(id=21), Row(id=26), Row(id=26), Row(id=30), Row(id=31), Row(id=33), Row(id=37), Row(id=37), Row(id=38), Row(id=41), Row(id=45), Row(id=46), Row(id=49), Row(id=50), Row(id=52), Row(id=59), Row(id=64), Row(id=65), Row(id=65), Row(id=66), Row(id=71), Row(id=72), Row(id=72), Row(id=72), Row(id=74), Row(id=77), Row(id=80), Row(id=81), Row(id=86), Row(id=88), Row(id=96), Row(id=98)]\n",
      "------------------\n",
      "[Row(id=0), Row(id=6), Row(id=8), Row(id=11), Row(id=13), Row(id=20), Row(id=27), Row(id=32), Row(id=35), Row(id=43), Row(id=46), Row(id=49), Row(id=52), Row(id=53), Row(id=58), Row(id=59), Row(id=62), Row(id=66), Row(id=67), Row(id=69), Row(id=71), Row(id=73), Row(id=75), Row(id=79), Row(id=86), Row(id=95), Row(id=97)]\n"
     ]
    }
   ],
   "source": [
    "# Sample withReplacement \n",
    "\n",
    "print(df.sample(True,0.3,7).collect()) # with Duplicates\n",
    "print(\"------------------\")\n",
    "print(df.sample(0.3,7).collect()) # No Duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d9922",
   "metadata": {},
   "source": [
    "# Stratified sampling in PySpark\n",
    "\n",
    "You can get Stratified sampling in PySpark without replacement by using sampleBy() method. It returns a sampling fraction for each stratum. If a stratum is not specified, it takes zero as the default.\n",
    "\n",
    "##### sampleBy() Syntax\n",
    "\n",
    "\n",
    "`sampleBy(col, fractions, seed=None)`\n",
    "\n",
    "col – column name from DataFrame\n",
    "\n",
    "fractions – It’s Dictionary type takes key and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ef7b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T10:57:21.157782Z",
     "start_time": "2023-12-02T10:57:12.860789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=0), Row(key=1), Row(key=1), Row(key=1), Row(key=1), Row(key=0), Row(key=1), Row(key=0), Row(key=1)]\n"
     ]
    }
   ],
   "source": [
    "df2=df.select((df.id%3).alias(\"key\"))\n",
    "print(df2.sampleBy(\"key\",{0:0.1,1:0.2},0).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce621ca0",
   "metadata": {},
   "source": [
    "PySpark RDD also provides sample() function to get a random sampling, it also has another signature takeSample() that returns an Array[T].\n",
    "\n",
    "## RDD sample() Syntax & Example\n",
    "\n",
    "PySpark RDD sample() function returns the random sampling similar to DataFrame and takes a similar types of parameters but in a different order.\n",
    "\n",
    "sample() of RDD returns a new RDD by selecting random sampling. Below is a syntax.\n",
    "\n",
    "`sample(self, withReplacement, fraction, seed=None)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413d8d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:05:19.323327Z",
     "start_time": "2023-12-02T11:05:19.292219Z"
    }
   },
   "outputs": [],
   "source": [
    "rdd=spark.sparkContext.range(0,100)\n",
    "print(rdd.sample(False,0.05,7).collect())\n",
    "print(\"-------\")\n",
    "print(rdd.sample(True,0.05,7).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352a893",
   "metadata": {},
   "source": [
    "RDD takeSample() Syntax & Example\n",
    "\n",
    "RDD takeSample() is an action hence you need to careful when you use this function as it returns the selected sample records to driver memory. Returning too much data results in an out-of-memory error similar to collect().\n",
    "\n",
    "#### Syntax of RDD takeSample() .\n",
    "\n",
    "\n",
    "`takeSample(self, withReplacement, num, seed=None)` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "210aea92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-02T11:11:03.431131Z",
     "start_time": "2023-12-02T11:11:03.417137Z"
    }
   },
   "outputs": [],
   "source": [
    "print(rdd.takeSample(False,10,7))\n",
    "\n",
    "print(rdd.takeSample(True,30,7))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
