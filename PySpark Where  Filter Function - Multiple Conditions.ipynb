{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a9db74",
   "metadata": {},
   "source": [
    "# PySpark `filter()` - \n",
    "        by Aishwarya Raut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7257cb3",
   "metadata": {},
   "source": [
    "# 1. PySpark DataFrame filter() Syntax\n",
    "\n",
    "`filter(condition)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6490e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T12:28:49.714330Z",
     "start_time": "2023-11-29T12:27:44.055119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SP').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10034ff3",
   "metadata": {},
   "source": [
    "# 2. DataFrame filter() with Column Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62c5756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T12:30:02.773640Z",
     "start_time": "2023-11-29T12:30:02.765640Z"
    }
   },
   "outputs": [],
   "source": [
    "# using equals condition\n",
    "df.filter(df.state==\"OH\").show(truncate=False)\n",
    "\n",
    "# not equals condition\n",
    "df.filter(df.state!= \"OH\").show(truncate=False)\n",
    "df.filter(~(df.state==\"OH\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SQL col() function \n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"state\") == \"OH\") \\\n",
    "    .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e8e94",
   "metadata": {},
   "source": [
    "# 3. DataFrame filter() with SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dacf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"gender == 'M'\").show()\n",
    "df.filter(\"gender != 'M'\").show()\n",
    "df.filter(\"gender <> 'M'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7ab7a",
   "metadata": {},
   "source": [
    "# 4. PySpark Filter with Multiple Conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbfbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.state==\"OH\") & (df.gender ==\"M\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5111e",
   "metadata": {},
   "source": [
    "# 5. Filter Based on List Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edffbf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter IS IN list values\n",
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df.filter(df.state.isin(li)).show()\n",
    "\n",
    "# filter NOT IS IN list values\n",
    "# There show all records with NY (NY is not part of the list)\n",
    "df.filter(~df.state.isin(li)).show()\n",
    "df.filter(df.state.isin(li)==False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff8eda",
   "metadata": {},
   "source": [
    "# 6. Filter Based on Starts With, Ends With, Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using startswith\n",
    "df.filter(df.state.startswith(\"N\")).show()\n",
    "\n",
    "#using endswith\n",
    "df.filter(df.state.endswith(\"H\")).show()\n",
    "\n",
    "#contains\n",
    "df.filter(df.state.contains(\"H\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8841db5",
   "metadata": {},
   "source": [
    "# 7. PySpark Filter like and rlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cda638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n",
    "     (4,\"Rames Rose\"),(5,\"Rames rose\")\n",
    "  ]\n",
    "\n",
    "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
    "\n",
    "# like - SQL LIKE pattern\n",
    "df2.filter(df2.name.like(\"%rose%\")).show()\n",
    "\n",
    "# rlike - SQL RLIKE pattern (LIKE with Regex)\n",
    "#This check case insensitive\n",
    "df2.filter(df2.name.rlike(\"(?i)^*rose$\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed02c0",
   "metadata": {},
   "source": [
    "# 8. Filter on an Array column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.languages,\"Java\")) \\\n",
    "    .show(truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35752d7c",
   "metadata": {},
   "source": [
    "# 9. Filtering on Nested Struct columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Struct condition\n",
    "df.filter(df.name.lastname == \"Williams\") \\\n",
    "    .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71773fa7",
   "metadata": {},
   "source": [
    "**What is the difference between where and filter in PySpark?**\n",
    "\n",
    "In PySpark, both filter() and where() functions are used to filter out data based on certain conditions. They are used interchangeably, and both of them essentially perform the same operation. where() is an alias for filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb3517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
