{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eca314",
   "metadata": {},
   "source": [
    "# PySpark UDF (User Defined Function) - \n",
    "        by Aishwarya Raut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f462461",
   "metadata": {},
   "source": [
    "# 1. Create PySpark UDF\n",
    "\n",
    "## 1.1 Create a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f28ce33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:21:33.128239Z",
     "start_time": "2023-12-01T09:20:52.956315Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark= SparkSession.builder.appName(\"SP\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad9d3e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:27:33.816435Z",
     "start_time": "2023-12-01T09:27:33.647499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns=[\"seq_no\",\"name\"]\n",
    "data=[(\"1\",\"kim namjoon\"),\n",
    "     (\"2\",\"kim seokjin\"),\n",
    "     (\"3\",\"min yoongi\")]\n",
    "\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.printSchema()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a8da0",
   "metadata": {},
   "source": [
    "## 1.2 Create a Python Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981f39f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:30:26.920749Z",
     "start_time": "2023-12-01T09:30:26.892765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function which take a string parameter\n",
    "# and converts the first letter of every word to \n",
    "# capital letter. \n",
    "\n",
    "def convertCase(str):\n",
    "    res_Str=\"\"\n",
    "    arr=str.split(\" \")\n",
    "    for x in arr:\n",
    "        res_Str=res_Str+x[0:1].upper()+x[1:len(x)]+\" \"\n",
    "    return res_Str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1372e",
   "metadata": {},
   "source": [
    "# 1.3 Convert a Python function to PySpark UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66f5bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:36:13.784744Z",
     "start_time": "2023-12-01T09:36:13.759129Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,udf \n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Converting function to UDF\n",
    "convertUDF= udf(lambda z: convertCase(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7063af8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:38:44.488474Z",
     "start_time": "2023-12-01T09:38:43.945195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Using UDF with DataFrame\n",
    "## 2.1 Using UDF with PySpark DF select()\n",
    "\n",
    "df.select(col(\"seq_no\"),\n",
    "         convertUDF(col(\"name\")).alias(\"Name\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2c8e6",
   "metadata": {},
   "source": [
    "# 2.2 Using UDF with PySpark DataFrame withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf7f9bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:39:55.057628Z",
     "start_time": "2023-12-01T09:39:55.036641Z"
    }
   },
   "outputs": [],
   "source": [
    "def uppercase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21a89335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-01T09:41:49.270248Z",
     "start_time": "2023-12-01T09:41:49.046775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seq_no: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- new_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uppercase_udf= udf(lambda x: uppercase(x),StringType())\n",
    "\n",
    "df.withColumn(\"new_name\",uppercase_udf(col(\"Name\"))).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b1fcf",
   "metadata": {},
   "source": [
    "# 2.3 Registering PySpark UDF and use it on SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Using UDF on SQL \"\"\"\n",
    "spark.udf.register(\"convertUDF\", convertCase,StringType())\n",
    "df.createOrReplaceTempView(\"NAME_TABLE\")\n",
    "spark.sql(\"select Seqno, convertUDF(Name) as Name from NAME_TABLE\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45464427",
   "metadata": {},
   "source": [
    "# 3. Creating UDF using annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db455816",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType()) \n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCase(col(\"Name\"))) \\\n",
    ".show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
